---
title: "A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation"
last_modified_at: 2018-01-03T09:45:06-05:00
classes: wide
---
<b>S. M. Lakew</b>, M. Cettolo, M. Federico. <i>Proceedings ofÂ  the 27th International Conference on Computational Linguistics</i>. Santa Fe, New Mexico, USA, 2018.

[[ArXiv]](https://arxiv.org/abs/1806.06957) [[Code]](https://github.com/surafelml/) [[Slide/Poster]](https://github.com/surafelml/)

## Abstract
Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.
